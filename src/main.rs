// External crates
use serde_json::json;
use ahash::RandomState;
use anyhow::{Error, Result};
use clap::{Parser, Subcommand};
use dashmap::{DashMap, DashSet};
use glob::glob;
use ndarray::Array1;
use rand::prelude::*;
use rand::{Rng, SeedableRng};
use rand_chacha::ChaCha20Rng;
use rayon::prelude::*;
use regex::Regex;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use serde_yaml;
use sha2::{Digest, Sha256};
use tiktoken_rs::{CoreBPE, p50k_base};
use unicode_segmentation::UnicodeSegmentation;

// Standard library
use std::collections::{HashMap, HashSet, VecDeque};
use std::fs::{create_dir_all, OpenOptions};
use std::hash::{DefaultHasher, Hash, Hasher};
use std::io::{BufRead, Write};
use std::option::Option;
use std::os::unix::fs::OpenOptionsExt;
use std::panic::catch_unwind;
use std::path::PathBuf;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::Instant;


// Internal crate imports
use mj_io::{expand_dirs, read_pathbuf_to_mem, write_mem_to_pathbuf, build_pbar};
use crate::storage::{compute_sig_size, FileMap, GenWriter, IntValueEnum, SignatureWriter, to_byte_size};
use crate::uf_rush2::UFRush;
pub mod storage;
pub mod uf_rush2;

const BIG_PRIME: u64 = 18446744073709551557;
const MAX_HASH: u64 = BIG_PRIME;

/*
New plan:
It helps to consider minHash in phases:

Phase 1: Look at all the file paths we want to run minHash dedup on and rename them
         with usize identifiers. Store this in a FileMap structure. 
         In this way, all documents are now uniquely identified by their 
         (path_id, line_num)

Phase 2: Compute (band_id, signature, doc_id) for every document.
         This is the heavy minhashing step, but can be parallelized 
         across chunks over the paths. 
            
         Essentially this works as follows -- for each path
         we loop over documents and compute it's minhash signatures. 
         We compute band_size * num_bands signatures, and then hash
         the signature across each band (rows in the pic below).
         If any two documents share the same (band_id, siganture), then
         they are duplicates.
         
                        Band Size
             ----------------------------
            |                            |
            |                            |
      Bands |                            |
            |                            |
            |                            |
            ------------------------------


Phase 3: Collect edges, linking all the documents that share the same 
         (band_id, siganture) together. Store these in a separate set of files.

Phase 4: Combine all the edges and build a GLOBAL union find data structure and 
         use this to collect connected components. Store both these connected 
         components (for later examination), and collect the lines that should
         be removed from the dataset (grouped by path_chunk, so they can be pruned
         in a multinode setting)

Phase 5: Clean the duplicate documents from the paths. Phase 4 tells us which 
         lines of each path should be removed.


(
Auxiliary phase: For examination purposes, we can look at each connected component
                  and get stats on the true pairwise jaccard similarity between 
                  documents marked as "duplicates"
)

----------------
Some design notes:

+ Config: Globally, this requires a json config. I'll make an example when
          I square up exactly what I need here.

+ Disk space: We rely heavily on storing auxiliary data structures on disk.
              Basically there's a state change after every phase where we make
              some new files after every phase. I'll build some python code 
              to read these for debuggy purposes later (TBD). Each section 
              will describe the file structure and contents in comments below.

+ Parallelism: Phase 3 and phase 4 need to be done globally (unfortunately, 
               there's no avoiding at least one global step). Phase 1 only 
               really needs access to the filenames but is very cheap.
               Phase 2, 4, and 5 are the heavy steps and can be done across
               file parallelism. This would require some coordination with the
               global data structures generated by phase 3 and 4.

+ s3: We don't touch s3 in rust. I have yet to find a package that works
      reliably, so let's just assume that all interaction between the LFS 
      and s3 is done outside the context of this rust code 


+ Input path structure: For simplicity, we require that the input paths all 
                        live in one directory and have unique basenames.


+ One fell swoop: if your dataset is small enough that you just want to do 
                  all phases in one command, use `min-hash`
*/



/*=================================================================
=                                  ARGS                           =
=================================================================*/

#[derive(Parser)]
#[clap(author, version, about, long_about = None)]
struct ArgParser {
    #[clap(subcommand)]
    command: Commands,

    #[arg(long, default_value_t=0)]
    threads: usize,
}

#[derive(Subcommand, Debug)]
enum Commands {
    #[clap(arg_required_else_help = true)]

    MinHash { 
        #[arg(required=true, long)]
        config: PathBuf,
     }, 


    BuildFileMap {
        #[arg(required=true, long)]
        config: PathBuf   
    },


    HashOnly {
        // Just runs and saves the hashes
        #[arg(required=true, long)]
        config: PathBuf,

        #[arg(long, default_value_t=0)]
        path_chunk: usize,

        #[arg(long, default_value_t=1)]
        num_path_chunks: usize
    },


    GatherEdges {
        // Saves the "edge" objects
        #[arg(required=true, long)]
        config: PathBuf
    },


    BuildEdges {
        #[arg(required=true, long)]
        config: PathBuf,

        #[arg(required=true, long)]
        sig_storage: PathBuf,

        #[arg(required=true, long)]
        group_storage: PathBuf,
    },


    BuildUf {
        // Gets a mapping from (path_id, line_num) -> cc_hash
        // Where the hash identifies the connected component
        #[arg(required=true, long)]
        config: PathBuf,

        #[arg(long, default_value_t=1)]
        num_path_chunks: usize,
    },


    UfSizePrune {
        // Prunes a dataset based on the connected component sizes:
        #[arg(required=true, long)]
        config: PathBuf, 

        #[arg(long, default_value_t=0)]
        path_chunk: usize,

        #[arg(long, default_value_t=1)]
        num_path_chunks: usize
    },

    TrueJacc {
        #[arg(required=true, long)]
        config: PathBuf
    },

    CountPL {
        #[arg(required=true, long)]
        input_dir: PathBuf,

        #[arg(long, required=true)]
        output: PathBuf
    },

    ConcatFilter {
        #[arg(required=true, long)]
        input_dir: PathBuf,

        #[arg(long, required=true)]
        output_dir: PathBuf
    },


    PythonFilter {
        #[arg(required=true, long)]
        input_dir: PathBuf,

        #[arg(required=true, long)]
        output_dir: PathBuf
    },

    LengthStats {
        #[arg(required=true, long)]
        input_dir: PathBuf,

        #[arg(required=true, long)]
        output: PathBuf,
    },


    TokenPl {
        #[arg(required=true, long)]
        data_dir: PathBuf,

        #[arg(required=true, long)]
        metadata_dir: PathBuf,

        #[arg(required=true, long)]
        output: PathBuf,
    },

    Subsample {
        #[arg(required=true, long)]
        input_dir: PathBuf,

        #[arg(required=true, long)]
        output_dir: PathBuf,

        #[arg(required=true, long)]
        ratio: f32
    }



}

/*=================================================================
=                             CONFIG                              =
=================================================================*/

#[derive(Debug, Serialize, Deserialize)]
struct Config {
    name: String,
    // Minhash parameters
    num_bands: usize,
    band_size: usize,
    ngram_size: usize,
    tokenizer_str: String,
    #[serde(default)]
    hash_seed: usize,

    // Engineery things
    num_sig_chunks: usize,
    num_docs: usize,
    max_lines_per_path: usize,
    content_key: String,

    // Local directories
    local_input: PathBuf,
    working_dir: PathBuf,
    output_dir: PathBuf,

    // Remote directories
    remote_input: PathBuf,
    remote_working_dir: PathBuf,
    remote_output: PathBuf,

    // Fancy options
    #[serde(default)]
    exact_override: bool,
    #[serde(default)]
    concat_key: Option<Vec<String>>

}

fn read_config(config_path: &PathBuf) -> Result<Config, Error> {
    let contents = read_pathbuf_to_mem(config_path).unwrap();
    let config: Config = serde_yaml::from_reader(contents).unwrap();
    Ok(config)
}




/*=================================================================
=                             UTILITIES                           =
=================================================================*/



fn get_concat_val(obj: &Value, concat_key: &Vec<String>) -> Result<Vec<String>, Error> {
    let mut concat_val: Vec<String> = Vec::new();

    for k in concat_key {
        concat_val.push(get_nested_json_val(obj, k).unwrap());
    }

    Ok(concat_val)
}


fn get_nested_json_val(obj: &Value, key: &String) -> Result<String, Error> {
    let mut current = obj;
    for subkey in key.split('.') {
        current = current.get(subkey).unwrap();
    }

    Ok(current.to_string())
}


struct OmniTokenizer {
    tokenizer_name: String,
    inner: CoreBPE
}

impl OmniTokenizer {
    fn new(tokenizer_name: &str) -> Result<Self, Error> {
        Ok(OmniTokenizer { tokenizer_name: tokenizer_name.to_string(), inner: p50k_base().unwrap()})
    }

    fn encode(&self, text: &str) -> Vec<usize> {
        match self.tokenizer_name.as_str() {
            "p50k" => {
                self.inner.encode_with_special_tokens(text)
            }
            "uniseg" => {
                text.split_word_bounds().map(|w| {
                    let mut hasher = DefaultHasher::new();
                    w.hash(&mut hasher);
                    hasher.finish() as usize
                }).collect()
            },
            _ => { // default to character level
                text.bytes().map(|b| b as usize).collect()
            },
        }
    }

}



/*=================================================================
=                      PHASE 1: BUILD FILE MAP                    =
=================================================================*/
/* GLOBAL STEP:

    This phase collects the paths (on the LFS) that we want to run minhash on.
    This creates and saves (as json) an object with the 
    - local_input: directory where the local files live
    - remote_input: direcotry (on s3, maybe) where these files were sourced from.
                    This is just for clean bookkeeping and not actually used.
    - indices: map from {local_filename -> path_id} where path_id is an integer.
               Life is way easier if each path just has a unique integral id.
*/


fn build_file_map(config: &PathBuf) -> Result<(), Error> {
    /*
    - Config is the path to the json minhash config
    This function reads all the files contained in the config.input path and creates a "filemap"
    which is a .json.gz that stores the 
    map from {file_name : pathbuf -> file_id : usize}
    */
    let config_contents = read_pathbuf_to_mem(config).unwrap();
    let config_value: serde_yaml::Value = serde_yaml::from_reader(config_contents).unwrap();

    let working_dir = PathBuf::from(config_value["working_dir"].as_str().unwrap());
    create_dir_all(&working_dir).unwrap();


    let local_input = PathBuf::from(config_value["local_input"].as_str().unwrap());
    let remote_input = PathBuf::from(config_value["remote_input"].as_str().unwrap());
    let file_map = FileMap::new(&local_input, &remote_input).unwrap();

    file_map.save(&working_dir.join("filemap.json.gz"))
}

/*=================================================================
=                      PHASE 2: HASH THE PATHS                    =
=================================================================*/
/* MULTINODE PARALLELISM ON PATH CHUNKS:
    
    This creates the signature files. The signature files are named like:
    working_dir/
    └── signatures/
       └── band_XXX/
           └── sigchunk_YYY/
               └── pathchunk_ZZZ.sig.bin

    Where the:
    - band_id (XXX) ranges from 0..num_bands (specified in the main config)
    - sigchunk_id (YYY) ranges from 0..num_sigchunks (specified in the main config)
    - pathchunk (ZZZ) ranges from 0..num_path_chunks (in the args of this function)

    
    And the contents of each file is a packing of bits of chunk size dependent on the number 
    of paths/maximum_line_size we have. But generally each file contains encoded-then-concatenated 
    triples of the form 
        `(signature, path_id, line_num)`
    where the exact size of each component can differ based on configs
*/



fn hash_only(config: &PathBuf, path_chunk: usize, num_path_chunks: usize) -> Result<(), Error> {
    println!("Starting part of Minhash run | config {:?} | chunk {:?}/{:?}", config, path_chunk, num_path_chunks);
    let start_main = Instant::now();    

    // Initialize everything we need to hash...
    let config_obj = read_config(config).unwrap();

    // -- Set up hashing stuff
    let band_seeds: Vec<u32> = _expand_band_seeds(&vec![config_obj.hash_seed as u32], config_obj.num_bands)
        .into_iter()
        .map(|x| x as u32)
        .collect();

    // -- Get files to hash
    let working_dir = config_obj.working_dir;
    let file_map = FileMap::load(&PathBuf::from(working_dir.clone()).join("filemap.json.gz")).unwrap();
    let local_input = file_map.local_input.clone();    
    let this_chunk = file_map.get_path_chunk(path_chunk, num_path_chunks);    

    // -- Handle storage stuff
    let sig_storage = working_dir.clone().join("sig_storage");
    create_dir_all(&sig_storage).unwrap();
    let num_sig_chunks = config_obj.num_sig_chunks;
    let signature_writer = SignatureWriter::new(&sig_storage, band_seeds.clone(), num_sig_chunks, path_chunk);
    let path_size = to_byte_size(file_map.indices.len());
    let line_size = to_byte_size(config_obj.max_lines_per_path);
    let sig_size = compute_sig_size(config_obj.num_docs);


    // And then loop through files and hash everything
    let start_hashing = Instant::now();
    let total_docs_hashed = AtomicUsize::new(0);    
    let hash_pbar = build_pbar(this_chunk.len(), "Paths");

    this_chunk.par_iter().for_each(|(path, path_id)| {
        let docs_hashed = process_path(&local_input.join(path), &band_seeds, *path_id, config_obj.band_size, config_obj.ngram_size, 
                                       config_obj.tokenizer_str.as_str(), &signature_writer, config_obj.num_sig_chunks, path_size, line_size, sig_size, &config_obj.content_key, 
                                       &config_obj.concat_key.clone(), config_obj.exact_override).unwrap();
        total_docs_hashed.fetch_add(docs_hashed, Ordering::SeqCst);
        hash_pbar.inc(1);
    });
    signature_writer.finish().unwrap();
    println!("(Chunk {:?}) ...collected all hashes in {:?} seconds", path_chunk, start_hashing.elapsed().as_secs());
    println!("-------------------------");
    println!("Completing part of Minhash run | config {:?} | chunk {:?}/{:?}", config, path_chunk, num_path_chunks);
    println!("Computed hashes for {:?} bands, {:?} docs", config_obj.num_bands, total_docs_hashed.into_inner());
    println!("Total runtime: {:?} (s)", start_main.elapsed().as_secs());
    Ok(())
}   


fn process_path(path: &PathBuf, band_seeds: &Vec<u32>, path_id: usize, band_size: usize, ngram_size: usize,
                 tokenizer_str: &str, signature_writer: &SignatureWriter, num_sig_chunks: usize,
                 path_size: usize, line_size: usize, sig_size: usize, content_key: &str,
                 concat_key: &Option<Vec<String>>, exact_override: bool) -> Result<usize, Error> {
    // Setup things: load data, build tokenizer, etc
    let data = read_pathbuf_to_mem(path).unwrap();
    // let mut buffer = Vec::new();
    // data.read_to_end(&mut buffer).unwrap();
    // println!("READ DATA {:?}", buffer);
    let tokenizer = OmniTokenizer::new(tokenizer_str).unwrap();
    let num_bands = band_seeds.len();
    let perm_seeds = _expand_band_seeds(&band_seeds, band_size);
    let path_id = IntValueEnum::new(path_id, path_size);

    // Grouped lines 
    let mut line_groups: HashMap<usize, Vec<String>> = HashMap::new();
    let mut last_concat_val: Vec<String> = Vec::new();
    let mut cur_line = 0;
    for (line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let json_obj: Value = serde_json::from_str(&line).expect(&format!("Failed to parse {:?} {:?}", path.clone(), line_num));
        let line_text = json_obj.get(content_key).unwrap().as_str().unwrap().to_string();
        if let Some(ref concat_key_real) = concat_key {
            let concat_val = get_concat_val(&json_obj, &concat_key_real).unwrap();
            if concat_val != last_concat_val {
                cur_line = line_num;
                last_concat_val = concat_val.clone();
            }
        } else {
            cur_line = line_num;
        }
        line_groups.entry(cur_line).or_default().push(line_text.to_string());
    }

    let mut groups_hashed = 0;
    for (k,v) in line_groups.into_iter() {
        let line_num = IntValueEnum::new(k, line_size);
        let text = v.join("\n");
        let hash_vals = if exact_override {
            let Ok(tokens) = catch_unwind(|| preprocess_text(&text, &tokenizer)) else {
                println!("Tokenization failed on {:?} | {:?} | {:?}", path.clone(), path_id, line_num.as_usize());
                continue;
            };
            get_hash_vals_from_tokens(tokens, &perm_seeds, ngram_size)
        } else {
            let n = perm_seeds.len();
            let mut hash_vals: Array1<u64>  = Array1::ones(n);
            hash_vals = hash_vals * (hash_object(&text) as u64);
            hash_vals
        };
        
        groups_hashed += 1;

        let bands = hash_vals.into_shape((num_bands, band_size)).unwrap();
        for (row, band_seed) in bands.rows().into_iter().zip(band_seeds.iter()) {
            let mut hasher = Sha256::new(); 
            hasher.update(bytemuck::cast_slice(row.as_slice().unwrap()));
            let hash = hasher.finalize();
            let band_signature = IntValueEnum::from_bytes(hash[..sig_size].to_vec(), sig_size);   
            _save_band_signature_to_disk(&signature_writer, *band_seed, band_signature, path_id.clone(), line_num.clone(), num_sig_chunks).unwrap();
        }
    }
    Ok(groups_hashed)
    
}


fn process_path_set(path: &PathBuf, path_id: usize, ngram_size: usize, tokenizer_str: &str, tokensets: &DashMap<(usize, usize), HashSet<usize>>) -> Result<(), Error> {
    let data = read_pathbuf_to_mem(path).unwrap();

    let tokenizer = OmniTokenizer::new(tokenizer_str).unwrap();
    for (line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let json: Value = serde_json::from_str(&line).expect(&format!("Failed to parse {:?} {:?}", path.clone(), line_num));        
        let text = json["text"].as_str().unwrap();
        let Ok(tokens) = catch_unwind(|| preprocess_text(text, &tokenizer)) else {
            println!("Tokenization failed on {:?} | {:?} | {:?}", path.clone(), path_id, line_num);
            continue;
        };
        let mut ngram: VecDeque<usize> = VecDeque::with_capacity(ngram_size);
        let mut ngram_count = 0;
        let mut doc_ngrams : HashSet<usize> = HashSet::new();
        for token in tokens {
            ngram.push_back(token);
            if ngram.len() >= ngram_size {
                ngram_count += 1;
                doc_ngrams.insert(hash_object(&ngram));
                ngram.pop_front();
            }
        }
        if ngram_count == 0 {
            doc_ngrams.insert(hash_object(&ngram));
        }
        tokensets.insert((path_id, line_num), doc_ngrams);
    }
    Ok(())
}


fn hash_object<T: Hash>(obj: &T) -> usize {
    let mut hasher = DefaultHasher::new();
    obj.hash(&mut hasher);
    hasher.finish() as usize
}



fn preprocess_text(text: &str, tokenizer: &OmniTokenizer) -> Vec<usize> 
{
    let text = clean_text(text);
    tokenizer.encode(&text)
}


fn clean_text(text: &str) -> String {
    // SlimPajama text cleaning process

    // Convert the document to lowercase
    let mut text = text.to_lowercase();

    // Remove punctuation
    let punctuation: &[_] = &['!', '"', '#', '$', '%', '&', '\'', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~'];
    text.retain(|c| !punctuation.contains(&c));

    // Replace multiple whitespace characters with a single space
    let re = Regex::new(r"\s+").unwrap();
    text = re.replace_all(&text, " ").to_string();

    // Trim leading and trailing whitespace
    text.trim().to_string()
}

fn get_hash_vals_from_tokens(tokens: Vec<usize>, perm_seeds: &Vec<u64>, ngram_size: usize) -> Array1<u64> {
    let a = _init_permutations(perm_seeds);
    let n = perm_seeds.len();

    let mut hash_vals = Array1::ones(n) * MAX_HASH;
    let mut ngram: VecDeque<usize> = VecDeque::with_capacity(ngram_size);
    let mut ngram_count = 0; 
    for token in tokens {
        ngram.push_back(token);
        if ngram.len() >= ngram_size {
            ngram_count += 1;
            hash_vals = _update_hash_vals(hash_vals, &a, &ngram);
            ngram.pop_front();
        }
    }
    hash_vals = if ngram_count == 0 {
        _update_hash_vals(hash_vals, &a, &ngram) // short document, still wanna hash it
    } else {
        hash_vals
    };

    hash_vals
}
    


fn _init_permutations(seeds: &Vec<u64>) -> Array1<u128> {
    // Initialize the permutations needed for each minhash
    let n = seeds.len();
    let mut a = Array1::zeros(n);
    for (i, &seed) in seeds.iter().enumerate() {
        let mut rng = ChaCha20Rng::seed_from_u64(seed);
        a[i] = rng.gen::<u128>() as u128;
    }
    a
}

#[allow(dead_code)]
fn rand_u64s(seed: u64, output_size: usize) -> Vec<u64> {
    let mut rng = StdRng::seed_from_u64(seed);
    let mut output: Vec<u64> = Vec::new();
    for _i in 0..output_size {
        output.push(rng.gen::<u64>());
    }
    output
}


fn _update_hash_vals(mut hash_vals: Array1<u64>, a: &Array1<u128>, ngram: &VecDeque<usize>) -> Array1<u64> {

    // hash the vecdeque as a u128 
    let hash_a = RandomState::with_seed(123);
    let hash_b = RandomState::with_seed(456);
    let hash_val_a = hash_a.hash_one(ngram);
    let hash_val_b = hash_b.hash_one(ngram);
    let cur_hash = ((hash_val_a as u128) << 64) | (hash_val_b as u128);

    // then multiply by a (mod 2^128) and take top 64 most significant bits
    let phv: Array1<u64> = a.mapv(|x| (x.wrapping_mul(cur_hash) >> 64) as u64);
    hash_vals.zip_mut_with(&phv, |x, y| *x = std::cmp::min(*x, *y));

    hash_vals

}

fn _expand_band_seeds(band_seeds: &Vec<u32>, band_size: usize) -> Vec<u64> {
    // Each "band seed" is expanded here to band_size random u64s, and flattened. (used to seed permutations)
    // Probably like no collisions here, so let's just not worry about that ;) 

    let mut perm_seeds: Vec<u64> = Vec::new();
    for band_seed in band_seeds.iter() {
        let mut rng = rand::rngs::StdRng::seed_from_u64(*band_seed as u64);
        for _i in 0..band_size {
            perm_seeds.push(rng.next_u64());
        }
    }
    perm_seeds
}

fn _save_band_signature_to_disk(signature_writer: &SignatureWriter, band_seed: u32, band_signature: IntValueEnum, 
                                path_id: IntValueEnum, line_num: IntValueEnum, num_sig_chunks: usize) -> Result<(), Error> {

    let sig_chunk = band_signature.as_usize() % num_sig_chunks;
    let contents = [band_signature.as_bytes(), path_id.as_bytes(), line_num.as_bytes()].concat();
    signature_writer.write_line(band_seed, sig_chunk, contents).unwrap();
    Ok(())
}



/*=================================================================
=                      PHASE 3: GATHER EDGES                      =
=================================================================*/
/* MULTINODE PARALLELISM ON BAND_ID 
    This creates the edge and singleton files.


    ----- edge files -----
    Edge file naming structure:
    working_dir/
    └── edges/
       └── sigchunk_YYY/
           └── band_XXX.edges.bin

    Where the:
        - band_id (XXX) ranges from 0..num_bands (pulled from signatures files)
        - sigchunk_id (YYY) ranges from 0..num_sigchunks (pulled from signatures files)

    And the contents of each file is a packed-bytes object, where if 
    (path_A, line_1) and (path_B, line_2) have the same signature in a single band, then the bytes

        `(path_A, line_1, path_B, line_2)` 
    
    appear in the file.


    ----- singleton file ----
    Singleton file naming structure:
    working_dir/
    └── edges/    
       └── singletons.bin    
    
    Where the contents just contain the number of lines per path, encoded as u64's like

        `(path_id, num_lines)`

*/

fn gather_edges(config: &PathBuf) -> Result<(), Error> {
    println!("Starting edge gather");
    let start_main = Instant::now();

    // Load the config and initialize things
    let config_obj = read_config(config).unwrap();
    let file_map = FileMap::load(&PathBuf::from(config_obj.working_dir.clone()).join("filemap.json.gz")).unwrap();
    let path_size = to_byte_size(file_map.indices.len());
    let line_size = to_byte_size(config_obj.max_lines_per_path);
    let sig_size = compute_sig_size(config_obj.num_docs);

    // Gather the files into the proper groups (which should live in the same hash-space-universe)
    let edge_groups = gather_groups(config_obj.working_dir.clone().join("sig_storage")).unwrap(); //(sigchunk, band_id) -> [(sigfile)]
    let single_band_id = edge_groups.iter().next().unwrap().key().1;
    let singleton_map : Option<DashMap<usize, usize>> = Some(DashMap::new()); // maps path_id -> max line_num 

    // Then build the cliques for each group of (sigchunk, band_id) -- across all files!

    println!("Starting edge collection...");
    let pbar = build_pbar(edge_groups.len(), "Band groups");
    edge_groups.par_iter()
        .for_each(|entry| {
            let (sigchunk, band_id) = entry.key();
            let singleton_map_opt = if *band_id == single_band_id {
                &singleton_map
            } else {
                &None
            };

            let band_group = build_band_group(entry.value(), sig_size, path_size, line_size, &singleton_map_opt).unwrap();
            let output_filename = config_obj.working_dir.clone()
                                  .join("edges")
                                  .join(format!("sigchunk_{:08}", sigchunk))
                                  .join(format!("band_{:08}.edges.bin", band_id));
            save_band_group(band_group, output_filename, path_size, line_size).unwrap();
            pbar.inc(1);
        });

    save_singletons(singleton_map.unwrap(), &config_obj.working_dir.clone().join("edges").join("singletons.bin")).unwrap();
    println!("... Gathered edges in {:?} seconds", start_main.elapsed().as_secs());
    // And save these for future use

    Ok(())
}

fn gather_groups(sig_storage: PathBuf) -> Result<DashMap<(usize, usize), Vec<PathBuf>>, Error> {
    let binding = sig_storage.clone().join("**").join("*.sig.bin");
    let sig_storage_str = binding.to_str().unwrap();
    let map: DashMap<(usize, usize), Vec<PathBuf>> = DashMap::new();
    for entry in glob(&sig_storage_str).unwrap() {
        let entry = entry.unwrap();
        let sigchunk_dir = entry
            .parent()
            .and_then(|p| p.file_name())
            .and_then(|name| name.to_str())
            .unwrap();
        let sigchunk = sigchunk_dir.split('_').last().unwrap().parse::<usize>().unwrap();
    
        let band_id_dir = entry
            .parent().unwrap().parent()
            .and_then(|p| p.file_name())
            .and_then(|name| name.to_str())
            .unwrap();
        let band_id = band_id_dir.split('_').last().unwrap().parse::<usize>().unwrap();

        map.entry((sigchunk, band_id)).or_default().push(entry);
    }    

    Ok(map)
}

fn build_band_group(band_sigs: &Vec<PathBuf>, sig_size: usize, path_size: usize, line_size: usize, 
                    singleton_map_opt: &Option<DashMap<usize, usize>>) -> 
    Result<Vec<Vec<(usize, usize)>>, Error> {
    // For a group of files that contain signatures within the same band (and a sig chunk)
    // Collects a list of (path_id: usize, line_id: usize) for each clique
    // (reading each file is done in parallel, so nothing upstream should be par_iter'ed)

    let entry_size = sig_size + path_size + line_size;

    // build map from signature -> [(path_id, line_num), ...]
    // to collect docs that have the same signature within this band
    let group_map : DashMap<IntValueEnum, Vec<(usize, usize)>> = DashMap::new();


    band_sigs.iter().for_each(|path| {
        let contents = read_pathbuf_to_mem(path).unwrap().into_inner().into_inner();
        contents.chunks(entry_size).for_each(|entry| {
            let sig = IntValueEnum::from_bytes(entry[..sig_size].to_vec(), sig_size);
            let path_id = IntValueEnum::from_bytes(entry[sig_size..sig_size+path_size].to_vec(), path_size).as_usize();
            let line_id = IntValueEnum::from_bytes(entry[sig_size+path_size..].to_vec(), line_size).as_usize();
            group_map.entry(sig).or_default().push((path_id, line_id));

            if let Some(singleton_map) = singleton_map_opt {
                singleton_map.entry(path_id).and_modify(|cur_max| {
                    if line_id > *cur_max {
                        *cur_max = line_id;
                    }
                }).or_insert(line_id);
            }
        });
    });
    
    // Select only the groups that have size > 1
    let band_group: Vec<Vec<(usize, usize)>> = group_map
        .into_iter()
        .map(|(_, group)| group)
        .filter(|value| value.len() >1)
        .collect();

    Ok(band_group)
}

fn save_band_group(band_group: Vec<Vec<(usize, usize)>>, output_file: PathBuf, path_size: usize, line_size: usize) -> Result<(), Error> {
    let max_path = IntValueEnum::new(1 << path_size - 1, path_size);
    let max_line = IntValueEnum::new(1 << line_size - 1, line_size);
    let group_end = [max_path.as_bytes(), max_line.as_bytes()].concat();


    if let Some(parent_dir) = output_file.parent() {
        if !parent_dir.exists() {
            create_dir_all(parent_dir).unwrap();
        }
    }
    let mut writer = OpenOptions::new()
                 .append(true)
                 .create(true)
                 .mode(0o644)
                 .open(output_file)
                 .unwrap();    

    band_group.into_iter()
        .for_each(|edgelist| {
            edgelist.into_iter()
                .for_each(|(path_id, line_num)| {
                    let contents = [IntValueEnum::new(path_id, path_size).as_bytes(),
                                    IntValueEnum::new(line_num, line_size).as_bytes()]
                                    .concat();
                    writer.write_all(&contents).unwrap();
        });
        writer.write(&group_end.clone()).unwrap();
    });
    writer.flush().unwrap();
    Ok(())
}
    


fn save_singletons(max_lines_per_doc: DashMap<usize, usize>, singleton_path: &PathBuf) -> Result<(), Error> {
    let data: Vec<u8> = max_lines_per_doc
        .par_iter()
        .flat_map(|r| {
            let key_bytes: u64 = (*r.key()) as u64;
            let val_bytes: u64 = (*r.value() + 1) as u64;
            vec![key_bytes.to_le_bytes(), val_bytes.to_le_bytes()]
        })
        .flat_map(|a| a)
        .collect();        

    write_mem_to_pathbuf(&data.as_slice(), singleton_path).unwrap();
    Ok(())    
}


/*=================================================================
=                      PHASE 4: Build Union Find                  =
=================================================================*/
/* NO MULTINODE PARALLELISM! =(
This creates a union find structure to merge together the edges. 
It creates the connected-components files and the kill files

    ----- cc files -----
    working_dir/
    └── ccs/
       └── chunk_ZZZ.cc.bin    

    where the chunk (ZZZ) ranges from 0..num_path_chunks and has contents, that when viewed as a single
    array are elements of the form 

        `(path_id, line_num)`

    saved as u64's, where there's a special token which is the u64::MAX repeated twice, indicating
    the end of a connected component


    ------ kill files ----
    working_dir/
    └── kill/
       └── chunk_ZZZ.kill.bin    

    where the chunk (ZZZ) ranges from 0..num_path_chunks and has contents, that when viewd as a single 
    array are is a concatenation of subarrays like

        `[path_id, kill_line_1, kill_line_2, ..., kill_line_N, terminus]`

    where each are u64s and terminus is u64::MAX
*/



fn build_uf(config: &PathBuf, num_path_chunks: usize) -> Result<(), Error> {
    // Takes the edges (saved as lists of lists of (path_id, line_num) pairs)
    // and builds a union find object and then collects CC's and saves a list of ccs.
    // Unless otherwise specified, also creates a list of to-delete lines, grouped by path_id

    println!("Building UnionFind...");
    let start_main = Instant::now();

    // Load the config to initialize things
    let config_obj = read_config(config).unwrap();
    let file_map = FileMap::load(&PathBuf::from(config_obj.working_dir.clone()).join("filemap.json.gz")).unwrap();
    let path_size = to_byte_size(file_map.indices.len());
    let line_size = to_byte_size(config_obj.max_lines_per_path);

    // Build the union find and unite all the edges
    let uf = UFRush::new();
    let all_edge_files = expand_dirs(vec![config_obj.working_dir.clone().join("edges")], Some(vec![".edges.bin"].as_slice())).unwrap();
    let singletons_path = config_obj.working_dir.clone().join("edges").join("singletons.bin");
    let singletons = load_singletons(&singletons_path).unwrap();

    add_singletons_to_uf(singletons, &uf, line_size).unwrap();
    let pbar = build_pbar(all_edge_files.len(), "Edge files");
    all_edge_files.into_par_iter().for_each(|p| {
        add_edge_file_to_uf(&p, &uf, path_size, line_size).unwrap();
        pbar.inc(1);
    });
    println!("Built unionfind in {:?} secs", start_main.elapsed().as_secs());

    // Run the cc collection
    println!("Starting CC Collection");
    let start_cc = Instant::now();
    let ccs = build_ccs(uf, line_size);
    println!("Built ccs in {:?} secs", start_cc.elapsed().as_secs());

    // Save the list of CCs in a writer object thing
    // First push these into a dashmap and then use the MAXPATH/MAXLINE paradigm
    println!("Organizing and saving ccs...");
    let cc_map : DashMap<usize, Vec<(usize, usize)>> = DashMap::new();
    let pbar = build_pbar(ccs.len(), "CC components");
    ccs.par_iter().for_each(|((path_id, line_num), key)| {
        cc_map.entry(*key).or_default().push((*path_id, *line_num));
        pbar.inc(1);
    });

    let cc_dir = config_obj.working_dir.clone().join("ccs");
    println!("CC DIR {:?}", cc_dir);
    save_all_ccs(&cc_map, &cc_dir, config_obj.num_sig_chunks).unwrap();
        
    let kill_list = collect_kill_list(cc_map);
    let kill_dir = config_obj.working_dir.clone().join("kill");
    save_kill_list(kill_list, &kill_dir, &file_map, num_path_chunks).unwrap();
    println!("Organized and saved ccs in {:?} secs", start_cc.elapsed().as_secs());


    println!("Finished all unionfind stuff in {:?} seconds" , start_main.elapsed().as_secs());
    Ok(())
}



fn add_edge_file_to_uf(edge_file: &PathBuf, uf: &UFRush, path_size: usize, line_size: usize) -> Result<(), Error> {
    let edge_data = read_pathbuf_to_mem(edge_file).unwrap().into_inner().into_inner();
    let max_path = IntValueEnum::new(1 << path_size - 1, path_size).as_usize();
    let max_line = IntValueEnum::new(1 << line_size - 1, line_size).as_usize();
    let group_end_id = pair2docid((max_path, max_line), line_size);
    let mut last_id = group_end_id;

    edge_data.chunks_exact(path_size + line_size).for_each(|c| {
        let path_id = IntValueEnum::from_bytes(c[..path_size].to_vec(), path_size).as_usize();
        let line_num = IntValueEnum::from_bytes(c[path_size..].to_vec(), line_size).as_usize();
        let cur_id = pair2docid((path_id, line_num), line_size);
        if cur_id != group_end_id && last_id != group_end_id {
            uf.unite(last_id, cur_id);
        }
        last_id = cur_id;
    });

    Ok(())
}

fn build_ccs(uf: UFRush, line_size: usize) -> Vec<((usize,usize), usize)> {
    let size = uf.nodes.len();

    let keys: Vec<usize> = uf.nodes.par_iter().map(|entry| *entry.key()).collect();
    println!("LEN KEYS IS {:?}", keys.len());
    println!("LINE SIZE IS {line_size}");
    let pbar = build_pbar(size, "Docs");
    keys.into_par_iter()
    .map(|key| {
        pbar.inc(1);
        (docid2pair(key, line_size), uf.find(key))
    })
    .collect()
}


fn pair2docid(pair: (usize, usize), line_size: usize) -> usize {
    // Given a (path_id, line_id) pair, converts it into a single usize 
    // (which is needed for UF rush)
    let (path_id, line_id) = pair;
    (path_id << (line_size * 8) ) + line_id
}

fn docid2pair(docid: usize, line_size: usize) -> (usize, usize) {
    // Inverse function of the pair2docid
    let mask = (1 << (line_size * 8)) - 1;
    (docid >> (line_size * 8), docid & mask)

}


fn load_singletons(singleton_path: &PathBuf) -> Result<Vec<(usize, usize)>, Error> {
    let data = read_pathbuf_to_mem(singleton_path).unwrap().into_inner().into_inner();

    let singletons: Vec<(usize, usize)> = data.chunks_exact(16).map(|c| {
        let path_id = u64::from_le_bytes(c[..8].try_into().unwrap()) as usize;
        let line_num = u64::from_le_bytes(c[8..].try_into().unwrap()) as usize;
        (path_id, line_num)
    }).collect();

    Ok(singletons)
    
}


fn add_singletons_to_uf(singletons: Vec<(usize, usize)>, uf: &UFRush, line_size: usize) -> Result<(), Error> {
    let pbar = build_pbar(singletons.len(), "Singleton paths");
    singletons.par_iter().for_each(|(path_id, max_line)| {
        for line_num in 0..(max_line+1) {
            let cur_id = pair2docid((*path_id, line_num), line_size);
            uf.find(cur_id);
        }
        pbar.inc(1);
    });
    Ok(())
}


fn save_all_ccs(cc_map: &DashMap<usize,Vec<(usize, usize)>>, cc_dir: &PathBuf, num_chunks: usize) -> Result<(), Error> {
    
    let writer = GenWriter::new(cc_dir, num_chunks, "cc");    
    let pbar = build_pbar(cc_map.len(), "Saving CCs");
    let max_u64_bytes = u64::MAX.to_le_bytes();

    let terminus: Vec<u8> = vec![max_u64_bytes, max_u64_bytes].into_iter().flat_map(|a| a).collect();
    cc_map.par_iter().for_each(|entry| {
        // Create contents before locking 
        let key = entry.key();
        let value = entry.value();
        if value.len() > 1 {
            let mut val_bytes: Vec<u8>= value.iter().flat_map(|r| {
                vec![(r.0 as u64).to_le_bytes(), 
                     (r.1 as u64).to_le_bytes()]
            }).flat_map(|a| a)
            .collect();
            val_bytes.extend(terminus.clone());
            writer.write_line(*key, val_bytes, 0).unwrap();
        }
        pbar.inc(1);
    });

    writer.finish().unwrap();
    Ok(())
}


fn load_all_ccs(cc_dir: &PathBuf) -> Result<DashMap<usize, Vec<(usize, usize)>>, Error> {

    let all_ccs: DashMap<usize, Vec<(usize,usize)>> = DashMap::new();
    let cc_paths = expand_dirs(vec![cc_dir.clone()], Some(vec![".cc.bin"].as_slice())).unwrap();
    let cc_id = AtomicUsize::new(0);
    let pbar = build_pbar(cc_paths.len(), "CC Files");

    cc_paths.iter().for_each(|p| {
        _load_single_cc(p, &all_ccs, &cc_id).unwrap();
        pbar.inc(1);
    });

    Ok(all_ccs)
}



fn _load_single_cc(path: &PathBuf, all_ccs: &DashMap<usize, Vec<(usize, usize)>>, cc_id: &AtomicUsize) -> Result<(), Error> {

    let contents = read_pathbuf_to_mem(path).unwrap().into_inner().into_inner();
    let chunk_size = 8 * 2; // 2x u64's
    let max_u64_bytes = u64::MAX.to_le_bytes();
    let terminus: Vec<u8> = vec![max_u64_bytes, max_u64_bytes].into_iter().flat_map(|a| a).collect();

    let mut last_chunk = terminus.clone();
    let mut cur_cc : Vec<(usize, usize)> = Vec::new();
    contents.chunks_exact(chunk_size).for_each(|c| {
        last_chunk = c.to_vec();
        let path_id = u64::from_le_bytes(c[..8].try_into().unwrap()) as usize;
        let doc_id = u64::from_le_bytes(c[8..].try_into().unwrap()) as usize;
        if c == terminus {
            let cc_name = cc_id.fetch_add(1, Ordering::SeqCst);
            all_ccs.entry(cc_name).or_default().extend(cur_cc.clone());
            cur_cc = Vec::new();
        } else {
            cur_cc.push((path_id, doc_id));
        }

    });

    
    Ok(())
}


fn collect_kill_list(cc_map: DashMap<usize, Vec<(usize, usize)>>) -> DashMap<usize, Vec<usize>> {
    // Creates map of path_id -> [line_num,...] for lines to remove from doc 
    let pbar = build_pbar(cc_map.len(), "Organizing kill ccs");
    let kill_list: DashMap<usize, Vec<usize>>  = DashMap::new();
    cc_map.par_iter().for_each(|entry| {
        let value: &Vec<(usize, usize)> = entry.value();
        if value.len() > 1 {
            for i in 0..value.len() - 1 {
                let (path_id, line_num) = value[i];
                kill_list.entry(path_id).or_default().push(line_num);
            }
        }
        pbar.inc(1);
    }); 

    kill_list
}


fn save_kill_list(kill_list: DashMap<usize, Vec<usize>>, kill_dir: &PathBuf, file_map: &FileMap, num_path_chunks: usize) -> Result<(), Error> {

    // Map path id to chunk id
    let path_id_2_chunk_id : DashMap<usize, usize> = DashMap::new();
    for chunk_id in 0..num_path_chunks {
        let path_chunk = file_map.get_path_chunk(chunk_id, num_path_chunks);
        path_chunk.par_iter().for_each(|entry| {
            let path_id = entry.1;
            path_id_2_chunk_id.insert(path_id, chunk_id);
        });        
    }

    // Then loop through kill list and write as we go:

    let writer = GenWriter::new(kill_dir, num_path_chunks, "kill");
    let pbar = build_pbar(kill_list.len(), "Writing kill list");
    let terminus = u64::MAX.to_le_bytes();
    kill_list.par_iter().for_each(|entry| {
        let path_id = entry.key();
        let chunk_id = path_id_2_chunk_id.get(path_id).unwrap();
        let line_nums: &Vec<usize> = entry.value();

        let mut contents : Vec<u8>  = Vec::new();
        contents.extend(path_id.to_le_bytes());
        let mut lines_concat: Vec<u8> = line_nums.into_iter().map(|i| i.to_le_bytes()).flat_map(|a| a).collect();
        lines_concat.extend(terminus.clone());
        contents.extend(lines_concat);
        writer.write_line(0, contents, *chunk_id).unwrap();
        pbar.inc(1);
    });
    writer.finish().unwrap();
    Ok(())
}


fn parse_kill_file(kill_file: &PathBuf) -> Result<DashMap<usize, Vec<usize>>, Error> {
    let kill_list: DashMap<usize, Vec<usize>> = DashMap::new();

    let contents = read_pathbuf_to_mem(kill_file).unwrap().into_inner().into_inner();
    let mut path_id: usize = 0;
    let mut restart = true;

    let terminus = u64::MAX.to_le_bytes();

    for chunk in contents.chunks_exact(8) {
        if chunk == terminus {
            restart = true;
            continue;
        }
        else if restart == true {
            path_id = u64::from_le_bytes(chunk.try_into().unwrap()) as usize;
            restart = false;
        } else {
            let line_num: usize = u64::from_le_bytes(chunk.try_into().unwrap()) as usize;
            kill_list.entry(path_id).or_default().push(line_num);
        }
    }

    Ok(kill_list)
}



/*=================================================================
=                      PHASE 5: CLEAN DUPLICATES                  =
=================================================================*/
/* MULTINODE PARALLELISM ON PATHS
    This cleans the duplicates out from the data. 
    It simply requires the outputs of the previous steps (namely the FileMap in step 1, and the 
    .kill.bin files in step 4).

    It will copy over (deduplicated) files from the input_dir to the output_dir (as specified by the config)

*/



fn uf_size_prune(config: &PathBuf, path_chunk: usize, num_path_chunks: usize) -> Result<(), Error> {
    println!("Starting UF-based pruning...");
    let start_main = Instant::now();
    let config_obj = read_config(config).unwrap();


    let working_dir = config_obj.working_dir; 
    let input_dir = config_obj.local_input; 
    let output_dir = config_obj.output_dir; 
    let file_map = FileMap::load(&PathBuf::from(working_dir.clone()).join("filemap.json.gz")).unwrap();
    let path_chunk_files = file_map.get_path_chunk(path_chunk, num_path_chunks);
    let kill_dir = working_dir.clone().join("kill");


    let concat_key = config_obj.concat_key; 

    // Parse the kill file into a map from path_id -> [lines to kill]
    println!("Reading kill file from disk...");
    let start_kill_read = Instant::now();
    let kill_file = GenWriter::get_filename(&kill_dir, path_chunk, "kill");
    let kill_list = parse_kill_file(&kill_file).unwrap();
    println!("Parsed kill file in {:?} seconds", start_kill_read.elapsed().as_secs());
    
    // Loop over the file map
    println!("Cleaning output");
    let start_kill_clean = Instant::now();
    let documents_removed = AtomicUsize::new(0);
    let documents_seen = AtomicUsize::new(0);


    let pbar = build_pbar(path_chunk_files.len(), "Files to clean");
    path_chunk_files.par_iter().for_each(|(path, path_id)| {
        let lines_to_kill = kill_list.entry(*path_id).or_default();
        let (remove, seen) = clean_path(&input_dir.clone().join(path), lines_to_kill.to_vec(), &input_dir, &output_dir, &concat_key).unwrap();        
        documents_removed.fetch_add(remove, Ordering::SeqCst);
        documents_seen.fetch_add(seen, Ordering::SeqCst);
        pbar.inc(1);
    });
    println!("Cleaned documents in {:?} secs", start_kill_clean.elapsed().as_secs());

    let documents_seen = documents_seen.into_inner();
    let documents_removed = documents_removed.into_inner();
    println!("Pruned data in {:?} secs", start_main.elapsed().as_secs());
    println!("Saw {:?} lines", documents_seen);
    println!("Removed {:?} lines", documents_removed);
    println!("Removal rate was {:?}", (documents_removed as f32) / (documents_seen as f32));
    Ok(())
}


fn get_output_filename(input_path: &PathBuf, config_input_dir: &PathBuf, config_output_dir: &PathBuf) -> Result<PathBuf, Error> {
    let replaced = input_path.clone()
        .strip_prefix(config_input_dir)
        .ok()
        .map(|stripped| config_output_dir.clone().join(stripped)).unwrap();
    Ok(replaced)
}


fn clean_path(path: &PathBuf, lines_to_kill: Vec<usize>, input_directory: &PathBuf, output_directory: &PathBuf,
              concat_key: &Option<Vec<String>>) -> Result<(usize, usize), Error> {
    // returns (lines_removed, lines_seen)
    let line_set: HashSet<usize> = lines_to_kill.into_iter().collect();
    let data = read_pathbuf_to_mem(path).unwrap();
    let mut concat_kill: HashSet<Vec<String>> = HashSet::new();

    let mut output_bytes = Vec::new();
    let mut line_num = 0;
    let mut lines_removed = 0;

    for line in data.lines() {
        let line = line?;
        if line_set.contains(&line_num) {
            lines_removed += 1;
            if let Some(concat_key_real) = concat_key {
                let line_json: Value = serde_json::from_str(&line).unwrap();
                let concat_val = get_concat_val(&line_json, &concat_key_real).unwrap();
                concat_kill.insert(concat_val);
            }

        } else {

            if let Some(concat_key_real) = concat_key {
                let line_json: Value = serde_json::from_str(&line).unwrap();
                let concat_val = get_concat_val(&line_json, &concat_key_real).unwrap();
                if concat_kill.contains(&concat_val) {
                    lines_removed += 1;
                } else {
                    output_bytes.extend(line.as_bytes());
                    output_bytes.push(b'\n');                    
                }
            }
        }
        line_num += 1;
    }

    if output_bytes.len() > 0 {
        let output_filename = get_output_filename(path, input_directory, output_directory).unwrap();
        write_mem_to_pathbuf(&output_bytes, &output_filename).unwrap();
    }

    Ok((lines_removed, line_num))
}



/*=================================================================
=                              Optional Analytics                 =
=================================================================*/
/*
Some optional analytics calls.

get_true_jacc_small: computes the actual jaccard similarity between all pairs of
                     documents in each connected component. 

    Saves outputs as a json list of lists:
    - each list represents a connected component
    - each element in each list is a (path_id_0, line_id_0, path_id_1, line_id_1, jaccard_sim)

*/
fn get_true_jacc_small(config: &PathBuf) -> Result<(), Error> {

    let start_main = Instant::now();
    println!("Starming true jaccard similarity calculations");
    let config_obj = read_config(config).unwrap();


    let file_map = FileMap::load(&config_obj.working_dir.join("filemap.json.gz")).unwrap();        
    let cc_dir = config_obj.working_dir.clone().join("ccs");

    // First load all ccs into memory and collect which files we have to load 
    println!("Loading cc's");
    let start_cc_load = Instant::now();
    let ccs = load_all_ccs(&cc_dir).unwrap();
    println!("Loaded ccs in {:?} secs", start_cc_load.elapsed().as_secs());


    // Then load all data that we need 
    println!("Starting path data load...");
    let start_path_load = Instant::now();
    // -- first collect the path ids we need
    let path_ids: DashSet<usize> = DashSet::new();
    ccs.par_iter().for_each(|entry| {
        let value = entry.value();
        for el in value {
            path_ids.insert(el.0);
        }
    });
    // -- and get  the path_id -> pathbuf lookup
    let reverse_path_indices: DashMap<usize, PathBuf> = DashMap::new();
    file_map.indices.iter().for_each(|entry| {
        reverse_path_indices.insert(entry.1.clone(), entry.0.clone());
    });
    
    // -- then load the data and get (path_id, line_num) -> ngram_set for all data
    let pbar = build_pbar(path_ids.len(), "Path ids");
    let tokensets: DashMap<(usize, usize), HashSet<usize>> = DashMap::new();    
    println!("WARNING: PATH SET NOT APPLICABLE FOR CONCAT_KEY [REPO/NAME] STUFF!!!");
    path_ids.into_par_iter().for_each(|p_id| {
        let path = config_obj.local_input.clone().join(reverse_path_indices.get(&p_id).unwrap().clone());
        process_path_set(&path, 
                         p_id, 
                         config_obj.ngram_size,
                         config_obj.tokenizer_str.as_str(),
                         &tokensets).unwrap();
        pbar.inc(1);
    });

    // -- and group it by cc
    let cc_ngrams : DashMap<usize, Vec<HashSet<usize>>> = DashMap::new();
    ccs.into_par_iter().for_each(|entry| {
        let cc_id = entry.0;
        for doc_id in entry.1 {
            let ngram_set = tokensets.remove(&doc_id).unwrap().1;
            cc_ngrams.entry(cc_id).or_default().push(ngram_set);
        }
    });
    println!("Loaded all path data in {:?} secs", start_path_load.elapsed().as_secs());


    // Then get all pairwise cc distances
    println!("Starting pairwise checks...");
    let start_pairwise = Instant::now();
    let pairwise_jacs = collect_pairwise_jaccards(cc_ngrams);
    println!("Got all pairwise distances in {:?} secs", start_pairwise.elapsed().as_secs());

    // Then save pairwise jaccard similarities
    println!("Saving pairwise similarities");
    let start_save = Instant::now();
    println!("PAIRWISE SIMS {:?}", pairwise_jacs);
    let pairwise_values: Vec<Vec<f32>> = pairwise_jacs.into_par_iter()
        .map(|entry| entry.1)
        .collect();
    let pairwise_path = config_obj.working_dir.clone().join("jaccard_values.json.gz");
    let pairwise_values_json = serde_json::to_vec(&pairwise_values).unwrap();
    write_mem_to_pathbuf(&pairwise_values_json, &pairwise_path).unwrap(); 
    println!("Saved pairwise similarities in {:?} secs", start_save.elapsed().as_secs());

    println!("Collected all true jaccard similarities in {:?} secs", start_main.elapsed().as_secs());
    Ok(())
}


fn collect_pairwise_jaccards(cc_ngrams: DashMap<usize, Vec<HashSet<usize>>>) -> DashMap<usize, Vec<f32>> {
    let pairwise_jacs: DashMap<usize, Vec<f32>> = DashMap::new();
    cc_ngrams.into_par_iter().for_each(|entry| {
        let cc_id = entry.0;
        for i in 0..entry.1.len() {
            for j in (i+1)..entry.1.len() {
                let sim = jaccard_similarity(&entry.1[i], &entry.1[j]);
                pairwise_jacs.entry(cc_id).or_default().push(sim);

            }
        }
    });

    pairwise_jacs
}


fn jaccard_similarity(x: &HashSet<usize>, y: &HashSet<usize>)  -> f32 {
    let cap = x.intersection(y).count();
    let cup = x.union(y).count();   
    cap as f32 / cup as f32
}



/*=================================================================
=                             Aggregate commands                  =
=================================================================*/

fn minhash(config: &PathBuf) -> Result<(), Error> {
    // Note: this is only for SMALL runs. We set some hyperparameters for you, and this isn't optimized for these use cases

    build_file_map(&config).unwrap();
    hash_only(&config, 0, 1).unwrap();
    gather_edges(&config).unwrap();
    build_uf(&config, 1).unwrap();
    uf_size_prune(&config, 0, 1).unwrap();

    Ok(())

}


/*======================================================================
=                               COUNT PROGRAMMING LANGUAGE             =
======================================================================*/

fn count_pl(input_dir: &PathBuf, output: &PathBuf) -> Result<(), Error> {

    let all_files = expand_dirs(vec![input_dir.clone()], None).unwrap();

    let global_count: DashMap<String, usize> = DashMap::new();
    let pbar = build_pbar(all_files.len(), "Files");
    
    all_files.par_iter().for_each(|p| {
        count_pl_single(p, &global_count).unwrap();
        pbar.inc(1);
    });


    println!("global_count {:?}", global_count);
    let global_count_hash : HashMap<String, usize> = global_count.iter().map(|r| (r.key().clone(), r.value().clone())).collect();
    let json = serde_json::to_string_pretty(&global_count_hash)?;

    write_mem_to_pathbuf(json.as_bytes(), output).unwrap();

    Ok(())
}


fn count_pl_single(path: &PathBuf, count: &DashMap<String, usize>) -> Result<(), Error> {
    let data = read_pathbuf_to_mem(path).unwrap();
    let mut single_hash : HashMap<String, usize> = HashMap::new();
    for (line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let json_obj: Value = serde_json::from_str(&line).expect(&format!("Failed to parse {:?} {:?}", path.clone(), line_num));
        let pl = json_obj.get("metadata").unwrap().get("language").unwrap().as_str().unwrap().to_string();

        *single_hash.entry(pl).or_insert(0) += 1;
    }

    single_hash.iter().for_each(|(k, v)| {
        *count.entry(k.clone()).or_insert(0).value_mut() += v;
    });

    Ok(())
}


/*======================================================================
=                               CONCATENATE AND FILTER                 =
======================================================================*/

const PROGRAMMING_LANGUAGES: &[&str] = &[
    "Python",
    "Java",
    "C++",
    "C",
    "JavaScript",
    "PHP",
    "C#",
    "Go",
    "TypeScript",
    "SQL",
    "Ruby",
    "Rust",
    "Jupyter Notebook",
    "Scala",
    "Kotlin",
    "Shell",
    "Dart",
    "Swift",
];


const JUST_PYTHQN: &[&str] = &[
    "Python"
];


fn concat_filter(input_dir: &PathBuf, output_dir: &PathBuf) -> Result<(), Error> {
    let start_main = Instant::now();
    let all_files = expand_dirs(vec![input_dir.clone()], None).unwrap();

    let pbar = build_pbar(all_files.len(), "Files");

    all_files.par_iter().for_each(|p| {
        let output_file = p.clone().strip_prefix(input_dir).ok().map(|stripped| output_dir.clone().join(stripped)).unwrap();


        concat_filter_single(p, &output_file).unwrap();
        pbar.inc(1);
    });
    println!("FINISHED THE THING IN {:?} SEC", start_main.elapsed().as_secs());
    Ok(())
}


fn concat_filter_single(input: &PathBuf, output: &PathBuf) -> Result<(), Error> {

    let pl_set : HashSet<&str> = PROGRAMMING_LANGUAGES.iter().copied().collect();
    let data = read_pathbuf_to_mem(input).unwrap();   
    //let mut repo_pl : HashMap<(String, String), Vec<Value>> = HashMap::new();

    let mut concat_texts : HashMap<(String, String), String> = HashMap::new();

    // Step 1: group all repo + pl's into vectors
    for (line_num, line) in data.lines().enumerate() {
        // Handle each operation that could fail, continuing on error
        let line = match line {
            Ok(l) => l,
            Err(e) => {
                eprintln!("Error reading line {}: {}", line_num, e);
                continue;
            }
        };

        let json_obj: Value = match serde_json::from_str(&line) {
            Ok(obj) => obj,
            Err(e) => {
                eprintln!("Failed to parse JSON at line {}: {}", line_num, e);
                continue;
            }
        };

        // Use if let chains to handle the nested gets more elegantly
        let (pl, repo) = if let (Some(_metadata), Some(lang), Some(repo_name)) = (
            json_obj.get("metadata"),
            json_obj.get("metadata").and_then(|m| m.get("language")).and_then(|l| l.as_str()),
            json_obj.get("metadata").and_then(|m| m.get("repo_name")).and_then(|r| r.as_str())
        ) {
            (lang.to_string(), repo_name.to_string())
        } else {
            eprintln!("Missing required fields at line {}", line_num);
            continue;
        };

        if !pl_set.contains(&pl.as_str()) {
            continue;
        }

        //concat_texts.entry((pl, repo)).or_default().push_str(json_obj.get("text").unwrap().as_str().unwrap());
    }

    return Ok(());
    let mut output_bytes: Vec<u8> = Vec::new();
    concat_texts.into_iter().for_each(|(k, v)| {
        //let id = v[0].get("id").unwrap();
        //let texts: Vec<&str> = v.iter().map(|d| {
        //    d.get("text").unwrap().as_str().unwrap()
        //}).collect();
        //let concat_text = texts.join("\n\n");
        let new_doc = serde_json::to_vec(&json!({
            "id": 1234,
            "text": v,
            "language": k.0,
            "repo_name": k.1,
            "num_files": v.len()
        })).unwrap();
        /*
        let new_doc = json!({
            "id": id, 
            "text": concat_text,
            "language": k.0,
            "repo_name": k.1,
            "num_files": v.len()
        }).to_string().into_bytes();
        */
        output_bytes.extend(new_doc);
        output_bytes.push('\n' as u8);  
    });
    write_mem_to_pathbuf(&output_bytes, output).unwrap();


    Ok(())
}

fn python_filter(input_dir: &PathBuf, output_dir: &PathBuf) -> Result<(), Error> {
    let start_main = Instant::now();
    let all_files = expand_dirs(vec![input_dir.clone()], None).unwrap();

    let pbar = build_pbar(all_files.len(), "Files");

    all_files.par_iter().for_each(|p| {
        let output_file = p.clone().strip_prefix(input_dir).ok().map(|stripped| output_dir.clone().join(stripped)).unwrap();


        python_filter_single(p, &output_file).unwrap();
        pbar.inc(1);
    });
    println!("FINISHED THE THING IN {:?} SEC", start_main.elapsed().as_secs());
    Ok(())
}


fn python_filter_single(input_path: &PathBuf, output_file: &PathBuf) -> Result<(), Error> {
    let data = read_pathbuf_to_mem(input_path).unwrap();

    let mut output_lines: Vec<_> = Vec::new();


    for (line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let json_obj: Value = serde_json::from_str(&line).expect(&format!("Failed to parse {:?} {:?}", input_path.clone(), line_num));
        let pl = json_obj.get("metadata").and_then(|m| m.get("language")).and_then(|l| l.as_str()).unwrap();
        if pl ==  "Python" {
            output_lines.extend(line.into_bytes());
            output_lines.push(b'\n');
        }
    }

    if output_lines.len() > 0 {
        write_mem_to_pathbuf(&output_lines.as_slice(), output_file).unwrap()
    }
    Ok(())
}


/*=====================================================
=                      LENGTH STATS                   =
=====================================================*/




fn length_stats(input_dir: &PathBuf, output: &PathBuf) -> Result<(), Error> {
    let start_main = Instant::now();
    let all_files = expand_dirs(vec![input_dir.clone()], None).unwrap();

    let pbar = build_pbar(all_files.len(), "Files");

    let all_lengths: Vec<usize> = all_files.par_iter().flat_map(|p| {
        let lengths = get_lengths(p).unwrap();
        pbar.inc(1);
        lengths
    }).collect();

    println!("Casting to str and saving");
    let output_str = all_lengths.par_iter().map(|x| x.to_string()).collect::<Vec<String>>().join(" ");
    let output_bytes = output_str.as_bytes();

    write_mem_to_pathbuf(output_bytes, output).unwrap();

    println!("FINISHED THE THING IN {:?} SEC", start_main.elapsed().as_secs());
    Ok(())
}

fn get_lengths(path: &PathBuf) -> Result<Vec<usize>, Error> {
    let data = read_pathbuf_to_mem(path).unwrap();
    let mut lengths: Vec<usize> = Vec::new();
    for (line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();    
        let json_obj: Value = serde_json::from_str(&line).expect(&format!("Failed to parse {:?} {:?}", path.clone(), line_num));
        let line_text = json_obj.get("text").unwrap().as_str().unwrap().to_string();
        lengths.push(line_text.len());
    }
    Ok(lengths)
}

/*================================================================
=                         Token PL                               =
================================================================*/

fn token_pl(data_dir: &PathBuf, metadata_dir: &PathBuf, output_file: &PathBuf) -> Result<(), Error> {

    let start_main = Instant::now();
    let data_files = expand_dirs(vec![data_dir.clone()], None).unwrap();

    let data_pbar = build_pbar(data_files.len(), "Data Files");

    let pl_lookup : DashMap<String, DashMap<usize, String>> = DashMap::new();

    data_files.par_iter().for_each(|p| {
        build_pl_lookup(p, &pl_lookup).unwrap();
        data_pbar.inc(1);
    });

    println!("Finished building token lookup in {:?} s", start_main.elapsed().as_secs());
    let start_csv = Instant::now();
    let metadata_ext = vec!["csv.gz"];
    let csv_files = expand_dirs(vec![metadata_dir.clone()], Some(metadata_ext.as_slice())).unwrap();
    let tokens_by_pl : DashMap<String, usize> = DashMap::new();
    let csv_pbar = build_pbar(csv_files.len(), "CSV Files");
    csv_files.par_iter().for_each(|p| {
        count_pl_sizes(p, &pl_lookup, &tokens_by_pl).unwrap();
        csv_pbar.inc(1);
     });
    println!("Finished csv parse in {:?}" , start_csv.elapsed().as_secs());

    let tokens_by_pl_hash : HashMap<String, usize> = tokens_by_pl.iter().map(|e| (e.key().clone(), e.value().clone())).collect();
    println!("COUNTS {:?}", tokens_by_pl_hash);
    let json = serde_json::to_value(tokens_by_pl_hash).unwrap();
    let json_bytes = serde_json::to_vec(&json).unwrap();
    write_mem_to_pathbuf(&json_bytes.as_slice(), &output_file).unwrap();

    Ok(())
}


fn build_pl_lookup(path: &PathBuf, pl_lookup: &DashMap<String, DashMap<usize, String>>) -> Result<(), Error> {

    let data = read_pathbuf_to_mem(path).unwrap();
    let filename = path.file_name().unwrap().to_string_lossy().into_owned();
    let cur_dash : DashMap<usize, String> = DashMap::new();
    for (line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let json_obj: Value = serde_json::from_str(&line).expect(&format!("Failed to parse {:?} {:?}", path.clone(), line_num));
        let pl = json_obj.get("metadata").unwrap().get("language").unwrap().as_str().unwrap().to_string();
        cur_dash.insert(line_num, pl.clone());
    }

    pl_lookup.insert(filename, cur_dash);
    Ok(())
}


fn count_pl_sizes(path :&PathBuf, pl_lookup: &DashMap<String, DashMap<usize, String>>, tokens_by_pl: &DashMap<String, usize>) -> Result<(), Error> {

    let data =read_pathbuf_to_mem(path).unwrap();
    let mut cur_map: HashMap<String, usize> = HashMap::new();

    for (_, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let parts: Vec<&str> = line.split(',').collect();
        let start_offset: usize = parts[0].parse().unwrap();
        let end_offset: usize = parts[1].parse().unwrap();
        let data_file = PathBuf::from(parts[3].to_string()).file_name().unwrap().to_string_lossy().into_owned();

        let line_number: usize = parts[4].parse().unwrap();

        let num_tokens = end_offset - start_offset;
        let inner = pl_lookup.get(&data_file).unwrap();
        let pl = inner.get(&line_number).unwrap().to_string();
        cur_map.entry(pl).and_modify(|v| *v += num_tokens).or_insert(num_tokens);
    }

    cur_map.iter().for_each(|(k, v)| {
        tokens_by_pl.entry(k.to_string()).and_modify(|og_val| *og_val += v).or_insert(*v);
    });

    Ok(())
}

/*=================================================================
=                           PARTITION TOKENS                      =
=================================================================*/


fn partition_npy_tokens(input_dir: &PathBuf, output_dir: &PathBuf, mb_size: usize) -> Result<(), Error> {

    let start_main = Instant::now();
    let data_files = expand_dirs(vec![input_dir.clone()], None).unwrap();
    let pbar = build_pbar(data_files.len(), "NPYS");
    for data_file in data_files {
        let output_file_base = data_file.clone().strip_prefix(input_dir).ok().map(|stripped| output_dir.clone().join(stripped)).unwrap();        
        partition_npy_tokens_single(&data_file, &output_file_base, mb_size).unwrap(); 
        pbar.inc(1);
    }

    println!("Finished splitting all files in {:?} secs", start_main.elapsed());
    Ok(())
}


fn partition_npy_tokens_single(data_file: &PathBuf, output_file_base: &PathBuf, mb_size: usize) -> Result<(), Error> {
    let contents = read_pathbuf_to_mem(data_file).unwrap().into_inner().into_inner();
    for (line_num, line) in contents.lines();

    let chunk_token_size = mb_size / 4;
    let num_tokens = contents.len() / 4;
    let num_chunks = (num_tokens + chunk_token_size - 1) / chunk_token_size;
    (0..num_chunks).into_par_iter().for_each(|i| {
        let chunk_contents = &contents[i * chunk_token_size.. i * chunk_token_size + chunk_token_size];
        let file_stem = output_file_base.file_stem().unwrap().to_str().unwrap();
        let new_file_name = format!("{}-{:04}.npy", file_stem, i);
        let parent = output_file_base.parent().unwrap();
        let output_file = parent.join(new_file_name);

        write_mem_to_pathbuf(chunk_contents, &output_file).unwrap();
    });
    Ok(())

}

/*================================================================
=                           SUBSAMPLE                            =
================================================================*/
fn subsample(input_dir: &PathBuf, output_dir: &PathBuf, ratio: f32) -> Result<(), Error> {
    let start_main = Instant::now();
    let all_files = expand_dirs(vec![input_dir.clone()], None).unwrap();

    let pbar = build_pbar(all_files.len(), "Files");

    all_files.par_iter().for_each(|p| {
        let output_file = p.clone().strip_prefix(input_dir).ok().map(|stripped| output_dir.clone().join(stripped)).unwrap();


        subsample_single(p, &output_file, ratio).unwrap();
        pbar.inc(1);
    });
    println!("FINISHED THE THING IN {:?} SEC", start_main.elapsed().as_secs());
    Ok(())
}



fn subsample_single(data_file: &PathBuf, output_file: &PathBuf, ratio: f32) -> Result<(), Error> {
    let data = read_pathbuf_to_mem(data_file).unwrap();
    let mut output_bytes: Vec<u8> = Vec::new();

    for (_line_num, line) in data.lines().enumerate() {
        let line = line.unwrap();
        let mut rng = rand::thread_rng();
        let random_float = rng.gen::<f32>();
        if ratio <= random_float {
            output_bytes.extend(line.as_bytes());
            output_bytes.push(b'\n');
        }
    }
    if output_bytes.len() > 0 {
        write_mem_to_pathbuf(&output_bytes, output_file).unwrap()
    }
    Ok(())



}




/*=================================================================
=                                 MAIN                            =
=================================================================*/


fn main() {
    let args = ArgParser::parse();
    let threads = args.threads;
    if threads != 0 {
        std::env::set_var("RAYON_NUM_THREADS", threads.to_string());
    }

    let result = match &args.command {
        Commands::MinHash{config} => {
            minhash(config)
        }

        Commands::BuildFileMap{config} => {
            build_file_map(config)
        },


        Commands::HashOnly {config, path_chunk, num_path_chunks} => {
            hash_only(config, *path_chunk, *num_path_chunks)
        },

        Commands::GatherEdges {config} => {
            gather_edges(config)
        },

        Commands::BuildUf {config, num_path_chunks} => {
            build_uf(config, *num_path_chunks)
        },

        Commands::UfSizePrune {config, path_chunk, num_path_chunks} => {
            uf_size_prune(config, *path_chunk, *num_path_chunks)
        },

        Commands::TrueJacc {config} => {
            get_true_jacc_small(config)
        },

        Commands::CountPL {input_dir, output} => {
            count_pl(input_dir, output)
        },

        Commands::ConcatFilter {input_dir, output_dir} => {
            concat_filter(input_dir, output_dir)
        },

        Commands::LengthStats {input_dir, output} => {
            length_stats(input_dir, output)
        },

        Commands::TokenPl {data_dir, metadata_dir, output} => {
            token_pl(data_dir, metadata_dir, output)
        },

        Commands::PythonFilter {input_dir, output_dir} => {
            python_filter(input_dir, output_dir)
        }



        Commands::Subsample {input_dir, output_dir, ratio} => {
            subsample(input_dir, output_dir, *ratio)
        }


        _ => {Ok(())}

    };
    result.unwrap()
}
